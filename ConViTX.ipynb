{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ba9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf        \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512edf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(224,224,3),include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23066820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6647a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer): \n",
    "    def __init__(self, patch_size,**kwargs):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'patch_size': self.patch_size}\n",
    "\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c975df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim,**kwargs):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection_dim = projection_dim\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(input_dim=self.num_patches, output_dim=self.projection_dim)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'num_patches': self.num_patches,\n",
    "               'projection_dim':self.projection_dim} \n",
    "\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f869ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size =224\n",
    "patch_size = 7  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 48\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a7fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56dd175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(input_feature, ratio=4):\n",
    "    \"\"\"Contains the implementation of Squeeze-and-Excitation(SE) block.\n",
    "    As described in https://arxiv.org/abs/1709.01507.\n",
    "    \"\"\"\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature.shape[channel_axis]\n",
    "\n",
    "    se_feature = layers.GlobalAveragePooling2D()(input_feature)\n",
    "    se_feature = layers.Reshape((1, 1, channel))(se_feature)\n",
    "    assert se_feature.shape[1:] == (1,1,channel)\n",
    "    se_feature = layers.Dense(channel // ratio,\n",
    "                        activation='relu',\n",
    "                        kernel_initializer='he_normal',\n",
    "                        use_bias=True,\n",
    "                        bias_initializer='zeros')(se_feature)\n",
    "    assert se_feature.shape[1:] == (1,1,channel//ratio)\n",
    "    se_feature = layers.Dense(channel,\n",
    "                       activation='sigmoid',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       use_bias=True,\n",
    "                       bias_initializer='zeros')(se_feature)\n",
    "    assert se_feature.shape[1:] == (1,1,channel)\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        se_feature = Permute((3, 1, 2))(se_feature)\n",
    "\n",
    "    se_feature = layers.multiply([input_feature, se_feature])\n",
    "    return se_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_layers = base_model.get_layer('block_6_project_BN').output\n",
    "cnn_features = se_block(cnn_layers, ratio=4)\n",
    "# cnn_features = layers.GlobalAveragePooling2D()(cnn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727985ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create patches.\n",
    "patches = Patches(patch_size)(base_model.input)\n",
    "# Encode patches.\n",
    "encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "# # Create multiple layers of the Transformer block.\n",
    "for _ in range(transformer_layers):\n",
    "    # Layer normalization 1.\n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    # Create a multi-head attention layer.\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "    )(x1, x1)\n",
    "    # Skip connection 1.\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "    # Layer normalization 2.\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "    # MLP.\n",
    "    x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "    # Skip connection 2.\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "# Create a [batch_size, projection_dim] tensor.\n",
    "vit_layers = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "vit_features = layers.Reshape((32, 32, 48))(vit_layers)\n",
    "vit_features = layers.DepthwiseConv2D(kernel_size=3,strides=1,activation='relu',use_bias=True,padding= \"valid\")(vit_features)\n",
    "vit_features = layers.DepthwiseConv2D(kernel_size=3,strides=1,activation='relu',use_bias=True,padding= \"valid\")(vit_features)\n",
    "vit_features = layers.DepthwiseConv2D(kernel_size=3,strides=2,activation='relu',use_bias=True,padding= \"same\")(vit_features)\n",
    "vit_features = se_block(vit_features, ratio=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_fusion = layers.Concatenate(name='cam_layer1')([cnn_features, vit_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345cfa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
    "    \"\"\"Inverted ResNet block.\"\"\"\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\n",
    "    in_channels = K.int_shape(inputs)[channel_axis]\n",
    "    pointwise_conv_filters = int(filters * alpha)\n",
    "    # Ensure the number of filters on the last 1x1 convolution is divisible by\n",
    "    # 8.\n",
    "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
    "    x = inputs\n",
    "    prefix = \"block_{}_\".format(block_id)\n",
    "\n",
    "    if block_id:\n",
    "        # Expand with a pointwise 1x1 convolution.\n",
    "        x = layers.Conv2D(\n",
    "            expansion * in_channels,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            use_bias=False,\n",
    "            activation=None,\n",
    "            name=prefix + \"expand\",\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization(\n",
    "            axis=channel_axis,\n",
    "            epsilon=1e-3,\n",
    "            momentum=0.999,\n",
    "            name=prefix + \"expand_BN\",\n",
    "        )(x)\n",
    "        x = layers.ReLU(6.0, name=prefix + \"expand_relu\")(x)\n",
    "    else:\n",
    "        prefix = \"expanded_conv_\"\n",
    "\n",
    "    # Depthwise 3x3 convolution.\n",
    "    if stride == 2:\n",
    "        x = layers.ZeroPadding2D((1,1), name=prefix + \"pad\"\n",
    "        )(x)\n",
    "    x = layers.DepthwiseConv2D(\n",
    "        kernel_size=3,\n",
    "        strides=stride,\n",
    "        activation=None,\n",
    "        use_bias=False,\n",
    "        padding=\"same\" if stride == 1 else \"valid\",\n",
    "        name=prefix + \"depthwise\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=channel_axis,\n",
    "        epsilon=1e-3,\n",
    "        momentum=0.999,\n",
    "        name=prefix + \"depthwise_BN\",\n",
    "    )(x)\n",
    "\n",
    "    x = layers.ReLU(6.0, name=prefix + \"depthwise_relu\")(x)\n",
    "\n",
    "    # Project with a pointwise 1x1 convolution.\n",
    "    x = layers.Conv2D(\n",
    "        pointwise_filters,\n",
    "        kernel_size=1,\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        activation=None,\n",
    "        name=prefix + \"project\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=channel_axis,\n",
    "        epsilon=1e-3,\n",
    "        momentum=0.999,\n",
    "        name=prefix + \"project_BN\",\n",
    "    )(x)\n",
    "\n",
    "    if in_channels == pointwise_filters and stride == 1:\n",
    "        return layers.Add(name=prefix + \"add\")([inputs, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f851dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_fusion = _inverted_res_block(feature_fusion, filters=128, alpha=1.0, stride=2, expansion=6, block_id=12)\n",
    "feature_fusion = se_block(feature_fusion, ratio=4)\n",
    "gap_layer = layers.GlobalAveragePooling2D()(feature_fusion)\n",
    "predictions = layers.Dense(38, activation='softmax')(gap_layer)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
